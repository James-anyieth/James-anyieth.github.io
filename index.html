<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_d3fhrn3rjn0q-3{list-style-type:none}ul.lst-kix_d3fhrn3rjn0q-2{list-style-type:none}ul.lst-kix_d3fhrn3rjn0q-1{list-style-type:none}.lst-kix_d3fhrn3rjn0q-0>li:before{content:"\0025cf   "}ul.lst-kix_d3fhrn3rjn0q-0{list-style-type:none}ul.lst-kix_d3fhrn3rjn0q-7{list-style-type:none}ul.lst-kix_d3fhrn3rjn0q-6{list-style-type:none}ul.lst-kix_d3fhrn3rjn0q-5{list-style-type:none}.lst-kix_d3fhrn3rjn0q-1>li:before{content:"\0025cb   "}ul.lst-kix_d3fhrn3rjn0q-4{list-style-type:none}.lst-kix_d3fhrn3rjn0q-3>li:before{content:"\0025cf   "}.lst-kix_d3fhrn3rjn0q-4>li:before{content:"\0025cb   "}.lst-kix_d3fhrn3rjn0q-2>li:before{content:"\0025a0   "}ul.lst-kix_d3fhrn3rjn0q-8{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_d3fhrn3rjn0q-7>li:before{content:"\0025cb   "}.lst-kix_d3fhrn3rjn0q-6>li:before{content:"\0025cf   "}.lst-kix_d3fhrn3rjn0q-8>li:before{content:"\0025a0   "}.lst-kix_d3fhrn3rjn0q-5>li:before{content:"\0025a0   "}ol{margin:0;padding:0}table td,table th{padding:0}.c13{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c28{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Arial";font-style:normal}.c23{background-color:#ffffff;color:#161616;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c4{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c19{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:italic}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c25{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c18{padding-top:18pt;padding-bottom:8pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c21{padding-top:0pt;padding-bottom:8pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c20{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c16{padding-top:0pt;padding-bottom:8pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c15{padding-top:12pt;padding-bottom:12pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c24{text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c1{background-color:#ffffff;font-style:italic;color:#161616;font-weight:700}.c27{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c22{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c5{color:inherit;text-decoration:inherit}.c9{padding:0;margin:0}.c8{font-size:12pt}.c11{font-style:italic}.c26{font-weight:700}.c2{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c22 doc-content"><p class="c19 title" id="h.jip1dqih9ckm"><span class="c7">Pedestrian Detection and Tracking System</span></p><p class="c10"><span class="c3">James Anyieth Yuot</span></p><p class="c10"><span class="c3">University of Juba</span></p><p class="c10 c2"><span class="c3"></span></p><h1 class="c28" id="h.kqyc8zqcv8c7"><span class="c29">Abstract</span></h1><p class="c15"><span>Pedestrian monitoring is rapidly emerging as a key area of interest, as data on pedestrian flow is essential not only for improving accessibility in specific urban areas but also for optimizing systems such as transit networks and traffic signal controls.</span><span>This chapter presents the method employed for the design, development, training, and evaluation of a pedestrian detection and tracking system. The study begins with the data acquisition process, detailing the collection and curation of relevant datasets to ensure robust model training. The annotation workflow is streamlined using Roboflow, facilitating efficient labeling and preprocessing of images for supervised learning. The core detection framework leverages a modified YOLOv9 architecture, optimized for enhanced accuracy and real-time performance in identifying pedestrians within complex scenes. To ensure continuous tracking across frames, the system integrates ByteTrack, a high-performance multi-object tracking algorithm that associates detections over time while handling occlusions and identity switches. The performance evaluation involves rigorous testing on benchmark datasets, employing metrics such as precision, recall, mean average precision (mAP), and tracking accuracy to assess the system&rsquo;s effectiveness. Additionally, more studies may be conducted to analyze the contributions of individual model components. The proposed methodology aims to deliver a reliable and scalable solution for pedestrian detection and tracking, suitable for applications in surveillance, autonomous vehicles, and smart city infrastructure. The findings provide insights into the trade-offs between speed and accuracy, offering a foundation for future improvements in real-time object detection and tracking systems.</span></p><h1 class="c4" id="h.oik5awtvfarb"><span class="c29">Method Overview</span></h1><p class="c21"><span class="c8">This chapter outlines the methodology adopted for the design, development, training, and evaluation of the pedestrian detection and tracking system. It includes the data acquisition process, annotation workflow using Roboflow, architecture of the modified YOLOv9 model, integration with ByteTrack for object tracking, and performance evaluation procedures. Each step has been carefully planned to ensure that the system achieves real-time performance with high detection and tracking accuracy. Our model approach involves using modified pretrained YOLOv9 and ByteTrack to train the dataset as it provides</span><span class="c8 c23">&nbsp;a more scalable approach to real-time pedestrian detection, target object recognition tasks and object tracking. The following are visualization samples of pedestrian detection and tracking.</span></p><h2 class="c18" id="h.nfmth4lcbiak"><span class="c17">Pedestrian Detection</span></h2><p class="c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 513.50px; height: 286.96px;"><img alt="" src="images/image1.png" style="width: 513.50px; height: 286.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="A picture of pedestrians in a street"></span></p><p class="c16"><span class="c1">Figure 1: A picture of pedestrians in a walkway</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 512.50px; height: 288.93px;"><img alt="" src="images/image3.png" style="width: 512.50px; height: 288.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c11 c20">Figure 2: A picture of detected pedestrians with bounding boxes</span></p><p class="c0 c2"><span class="c12"></span></p><h2 class="c25" id="h.nhq17z9dov08"><span class="c17">Pedestrian Tracking</span></h2><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 515.00px; height: 354.86px;"><img alt="" src="images/image2.png" style="width: 515.00px; height: 354.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c11 c26">Figure 3: A picture of a pedestrian tracking on a crosswalk</span></p><h1 class="c4" id="h.i9qaqxkjlqe4"><span class="c29">Citation</span></h1><ul class="c9 lst-kix_d3fhrn3rjn0q-0 start"><li class="c13 li-bullet-0"><span class="c8 c11">Glen Jocher and Jing Qiu, Ultralytics YOLO11, </span><span class="c8 c11 c27"><a class="c5" href="https://www.google.com/url?q=https://github.com/ultralytics/ultralytics&amp;sa=D&amp;source=editors&amp;ust=1752148261740562&amp;usg=AOvVaw3QX9JJ4Ff_HHLkO2sRPfFX">https://github.com/ultralytics/ultralytics</a></span><span class="c8 c11">, Accessed 25 June 2025</span></li></ul><p class="c2 c14"><span class="c3"></span></p></body></html>